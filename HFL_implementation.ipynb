{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the MNIST dataset to work on\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "data_path_str = \"./data\"\n",
    "ETA = \"\\N{GREEK SMALL LETTER ETA}\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.deterministic=True\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # normalize by training set mean and standard deviation\n",
    "    #  resulting data has mean=0 and std=1\n",
    "    # transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = MNIST(data_path_str, train=True, download=True, transform=transform)\n",
    "test_dataset= MNIST(data_path_str, train=False, download= True, transform= transform)\n",
    "test_loader = DataLoader(\n",
    "    MNIST(data_path_str, train=False, download=True, transform=transform),\n",
    "    # decrease batch size if running into memory issues when testing\n",
    "    # a bespoke generator is passed to avoid reproducibility issues\n",
    "    shuffle=False, drop_last=False, batch_size=10000, generator=torch.Generator())\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning data (each image into 4 parts)\n",
    "\n",
    "data1= torch.stack([a[0:7]/255 for a in train_dataset.data])\n",
    "data2= torch.stack([a[7:14]/255 for a in train_dataset.data])\n",
    "data3= torch.stack([a[14:21]/255 for a in train_dataset.data])\n",
    "data4= torch.stack([a[21:28]/255 for a in train_dataset.data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "\n",
    "testA= torch.stack([a[0:7]/255 for a in test_dataset.data])\n",
    "testB= torch.stack([a[7:14]/255 for a in test_dataset.data])\n",
    "testC= torch.stack([a[21:28]/255 for a in test_dataset.data])\n",
    "testD= torch.stack([a[21:28]/255 for a in test_dataset.data])\n",
    "test_labels= [test_loader.dataset[i][1] for i in range(len(test_loader.dataset))]\n",
    "\n",
    "# Partitioning test dataset for each of the label owners\n",
    "testA_1= testA[0:2000]\n",
    "testB_1= testB[0:2000]\n",
    "testC_1= testC[0:2000]\n",
    "testD_1= testD[0:2000]\n",
    "test_labels1= test_labels[0:2000]\n",
    "\n",
    "testA_2= testA[2000:4000]\n",
    "testB_2= testB[2000:4000]\n",
    "testC_2= testC[2000:4000]\n",
    "testD_2= testD[2000:4000]\n",
    "test_labels2= test_labels[2000:4000]\n",
    "\n",
    "testA_3= testA[4000:6000]\n",
    "testB_3= testB[4000:6000]\n",
    "testC_3= testC[4000:6000]\n",
    "testD_3= testD[4000:6000]\n",
    "test_labels3= test_labels[4000:6000]\n",
    "\n",
    "testA_4= testA[6000:8000]\n",
    "testB_4= testB[6000:8000]\n",
    "testC_4= testC[6000:8000]\n",
    "testD_4= testD[6000:8000]\n",
    "test_labels4= test_labels[6000:8000]\n",
    "\n",
    "testA_5= testA[8000:10000]\n",
    "testB_5= testB[8000:10000]\n",
    "testC_5= testC[8000:10000]\n",
    "testD_5= testD[8000:10000]\n",
    "test_labels5= test_labels[8000:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating label owner split\n",
    "\n",
    "from typing import cast\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from torch.utils.data import Subset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def split(nr_clients: int, seed: int) -> list[Subset]:\n",
    "    rng = npr.default_rng(seed)\n",
    "    indices= rng.permutation(len(train_dataset))\n",
    "    splits = np.array_split(indices, nr_clients)\n",
    "\n",
    "    return [Subset(train_dataset, split) for split in cast(list[list[int]], splits)], indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating label split\n",
    "# Sample_split contains the labels after permuting the original label set\n",
    "# Sample_ids contains the permutation used for the randomization process\n",
    "\n",
    "sample_split, sample_ids= split(5, 42)\n",
    "\n",
    "label_owner1= sample_split[0]\n",
    "label_owner2= sample_split[1]\n",
    "label_owner3= sample_split[2]\n",
    "label_owner4= sample_split[3]\n",
    "label_owner5= sample_split[4]\n",
    "\n",
    "label_id1= sample_ids[0:12000]\n",
    "label_id2= sample_ids[12000:24000]\n",
    "label_id3= sample_ids[24000:36000]\n",
    "label_id4= sample_ids[36000:48000]\n",
    "label_id5= sample_ids[48000:60000]\n",
    "\n",
    "# Aligning the data across each of the owners and label owner 1\n",
    "# Retrieving data corresponding to which labels are with label owner 1\n",
    "\n",
    "labels1= [label_owner1[i][1] for i in range(len(label_owner1))]\n",
    "dataA_label1= torch.stack([data1[i] for i in label_id1])\n",
    "dataB_label1= torch.stack([data2[i] for i in label_id1])\n",
    "dataC_label1= torch.stack([data3[i] for i in label_id1])\n",
    "dataD_label1= torch.stack([data4[i] for i in label_id1])\n",
    "data_labels1= [dataA_label1, dataB_label1, dataC_label1, dataD_label1]\n",
    "\n",
    "\n",
    "# Doing the same for each of the other 4 label owners\n",
    "labels2= [label_owner2[i][1] for i in range(len(label_owner2))]\n",
    "dataA_label2= torch.stack([data1[i] for i in label_id2])\n",
    "dataB_label2= torch.stack([data2[i] for i in label_id2])\n",
    "dataC_label2= torch.stack([data3[i] for i in label_id2])\n",
    "dataD_label2= torch.stack([data4[i] for i in label_id2])\n",
    "data_labels2= [dataA_label2, dataB_label2, dataC_label2, dataD_label2]\n",
    "\n",
    "labels3= [label_owner3[i][1] for i in range(len(label_owner3))]\n",
    "dataA_label3= torch.stack([data1[i] for i in label_id3])\n",
    "dataB_label3= torch.stack([data2[i] for i in label_id3])\n",
    "dataC_label3= torch.stack([data3[i] for i in label_id3])\n",
    "dataD_label3= torch.stack([data4[i] for i in label_id3])\n",
    "data_labels3= [dataA_label3, dataB_label3, dataC_label3, dataD_label3]\n",
    "\n",
    "labels4= [label_owner4[i][1] for i in range(len(label_owner4))]\n",
    "dataA_label4= torch.stack([data1[i] for i in label_id4])\n",
    "dataB_label4= torch.stack([data2[i] for i in label_id4])\n",
    "dataC_label4= torch.stack([data3[i] for i in label_id4])\n",
    "dataD_label4= torch.stack([data4[i] for i in label_id4])\n",
    "data_labels4= [dataA_label4, dataB_label4, dataC_label4, dataD_label4]\n",
    "\n",
    "labels5= [label_owner1[i][1] for i in range(len(label_owner5))]\n",
    "dataA_label5= torch.stack([data1[i] for i in label_id5])\n",
    "dataB_label5= torch.stack([data2[i] for i in label_id5])\n",
    "dataC_label5= torch.stack([data3[i] for i in label_id5])\n",
    "dataD_label5= torch.stack([data4[i] for i in label_id5])\n",
    "data_labels5= [dataA_label5, dataB_label5, dataC_label5, dataD_label5]\n",
    "\n",
    "accuracy_at_each_epoch= []\n",
    "# training_loss= []\n",
    "# test_loss= []\n",
    "# epoch_nums= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Data owner neural network\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "class BottomModel(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat):\n",
    "        super(BottomModel, self).__init__()\n",
    "        self.local_out_dim = out_feat # Final output dimension of the bottom model\n",
    "        self.flatten= nn.Flatten(start_dim=1, end_dim=2)\n",
    "        # self.conv1= nn.Conv1d(in_feat, 32, 3, 1)\n",
    "        # self.conv2 = nn.Conv1d(32, 16, 3, 1)\n",
    "        self.lin1= nn.Linear(196, 66)\n",
    "        self.dropout= nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x:torch.tensor):\n",
    "        # x= self.conv1(x)\n",
    "        # x= self.conv2(x)\n",
    "        x= self.flatten(x)\n",
    "        x= F.relu(self.lin1(x))\n",
    "        # x= F.max_pool2d(x, 2)\n",
    "        x= self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label owner neural network\n",
    "\n",
    "class TopModel(nn.Module):\n",
    "    def __init__(self, local_models, n_outs):\n",
    "        super(TopModel, self).__init__()\n",
    "        # top_in_dim= sum([i.local_out_dim for i in local_models])\n",
    "        self.lin1 = nn.Linear(264, 100)\n",
    "        self.lin2 = nn.Linear(100, 10) # Final output = number of possible classes (10 digit types)\n",
    "        self.act = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        concat_outs = torch.cat(x, dim=1)  # concatenate local model outputs before forward pass\n",
    "        x = self.act(self.lin1(concat_outs))\n",
    "        x= F.relu(x)\n",
    "        x = self.act(self.lin2(x))\n",
    "        x= self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VFLNetwork(nn.Module):\n",
    "    def __init__(self, local_models, n_outs):\n",
    "        super(VFLNetwork, self).__init__()\n",
    "        self.bottom_models = local_models # Shared set of bottom models for the entire process of training\n",
    "        self.top_models = [TopModel(self.bottom_models, n_outs) for _ in range(5)] # Creating 5 top models, one for each label owner \n",
    "        self.optimizers = [optim.AdamW(self.top_models[i].parameters()) for i in range(5)]\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.valid_owner= [0, 1, 2, 3, 4]\n",
    "        self.indices= [0]*5\n",
    "\n",
    "# Need to change the nature of x as well, it is going to be a list of lists (label as well as data partitioning done)\n",
    "\n",
    "    def train_with_settings(self, epochs, batch_sz, x, y):\n",
    "        num_batches = (12000 // batch_sz)*5 if 12000 % batch_sz == 0 else (12000 // batch_sz + 1)*5\n",
    "        for epoch in range(epochs):\n",
    "            for opt in self.optimizers:\n",
    "                opt.zero_grad()\n",
    "            self.valid_owner= [0, 1, 2, 3, 4]\n",
    "            self.indices= [0]*5\n",
    "            total_loss = 0.0\n",
    "            correct = 0.0\n",
    "            total = 0.0\n",
    "            for j in range(1000):\n",
    "                label_owner_id= npr.choice(self.valid_owner)\n",
    "                curr_data= x[label_owner_id]\n",
    "                curr_labels= y[label_owner_id]\n",
    "                x_minibatch = [p[int(self.indices[label_owner_id]):int(self.indices[label_owner_id] + batch_sz)] for p in curr_data]\n",
    "                y_minibatch = torch.tensor(curr_labels[int(self.indices[label_owner_id]):int(self.indices[label_owner_id] + batch_sz)], dtype=torch.long)\n",
    "                self.indices[label_owner_id]+= batch_sz\n",
    "                if(self.indices[label_owner_id]==12000):\n",
    "                    self.valid_owner.remove(label_owner_id)\n",
    "                \n",
    "                outs = self.forward(x_minibatch, label_owner_id)\n",
    "                pred= torch.argmax(outs, dim=1)\n",
    "                actual = y_minibatch\n",
    "                correct += torch.sum((pred == actual))\n",
    "                total += len(actual)\n",
    "                loss = self.criterion(outs, y_minibatch)\n",
    "                total_loss += loss\n",
    "                loss.backward()\n",
    "                self.optimizers[label_owner_id].step()\n",
    "\n",
    "            print(\n",
    "                f\"Epoch: {epoch} Train accuracy: {correct * 100 / total:.2f}% Loss: {total_loss.detach().numpy()/num_batches:.3f}\")\n",
    "            \n",
    "            if (epoch+1)%25== 0:\n",
    "                self.aggregation()\n",
    "            # accuracy_at_each_epoch.append(total_loss.detach().numpy()/num_batches)\n",
    "            # if epoch== epochs-1:\n",
    "            #     training_loss.append(total_loss.detach().numpy()/num_batches)\n",
    "\n",
    "    def forward(self, x, label_owner_id):\n",
    "        local_outs = [self.bottom_models[i](x[i]) for i in range(len(self.bottom_models))]\n",
    "        return self.top_models[label_owner_id](local_outs)\n",
    "\n",
    "    def test(self, x, y, label_owner_id): # Additional parameter to define which label owner's model is to be tested.\n",
    "        # Test set to be chosen according to the top model which is to be tested on (x and y contain complete sets that have been partitioned)\n",
    "        with torch.no_grad():\n",
    "            outs = self.forward(x[label_owner_id], label_owner_id)\n",
    "            pred = torch.argmax(outs, dim=1)\n",
    "            actual = torch.tensor(y[label_owner_id])\n",
    "            accuracy = torch.sum((pred == actual)) / len(actual)\n",
    "            loss = self.criterion(outs, actual)\n",
    "            return accuracy, loss\n",
    "\n",
    "    def aggregation(self):\n",
    "        parameter_set= []\n",
    "        avg_parameters= OrderedDict()\n",
    "        with torch.no_grad():\n",
    "            for i in range(5):\n",
    "                parameter_set.append(self.top_models[i].state_dict())\n",
    "            \n",
    "            for key in parameter_set[0]:\n",
    "                avg_parameters[key]= (parameter_set[0][key]+parameter_set[1][key]+parameter_set[2][key]+parameter_set[3][key] + parameter_set[4][key])/5\n",
    "            \n",
    "            for i in range(5):\n",
    "                self.top_models[i].load_state_dict(avg_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train accuracy: 33.62% Loss: 1.944\n",
      "Epoch: 1 Train accuracy: 44.94% Loss: 1.670\n",
      "Epoch: 2 Train accuracy: 42.64% Loss: 1.783\n",
      "Epoch: 3 Train accuracy: 37.16% Loss: 1.853\n",
      "Epoch: 4 Train accuracy: 31.44% Loss: 1.929\n",
      "Epoch: 5 Train accuracy: 29.38% Loss: 1.966\n",
      "Epoch: 6 Train accuracy: 28.83% Loss: 1.976\n",
      "Epoch: 7 Train accuracy: 32.48% Loss: 1.897\n",
      "Epoch: 8 Train accuracy: 39.92% Loss: 1.777\n",
      "Epoch: 9 Train accuracy: 47.04% Loss: 1.588\n",
      "Epoch: 10 Train accuracy: 49.17% Loss: 1.518\n",
      "Epoch: 11 Train accuracy: 50.99% Loss: 1.459\n",
      "Epoch: 12 Train accuracy: 53.86% Loss: 1.416\n",
      "Epoch: 13 Train accuracy: 54.07% Loss: 1.410\n",
      "Epoch: 14 Train accuracy: 54.50% Loss: 1.385\n",
      "Epoch: 15 Train accuracy: 57.83% Loss: 1.306\n",
      "Epoch: 16 Train accuracy: 59.25% Loss: 1.252\n",
      "Epoch: 17 Train accuracy: 63.05% Loss: 1.171\n",
      "Epoch: 18 Train accuracy: 64.20% Loss: 1.143\n",
      "Epoch: 19 Train accuracy: 65.28% Loss: 1.102\n",
      "Epoch: 20 Train accuracy: 67.12% Loss: 1.038\n",
      "Epoch: 21 Train accuracy: 67.26% Loss: 1.026\n",
      "Epoch: 22 Train accuracy: 67.27% Loss: 1.026\n",
      "Epoch: 23 Train accuracy: 67.16% Loss: 1.035\n",
      "Epoch: 24 Train accuracy: 67.27% Loss: 1.025\n",
      "Epoch: 25 Train accuracy: 27.50% Loss: 2.023\n",
      "Epoch: 26 Train accuracy: 40.80% Loss: 1.744\n",
      "Epoch: 27 Train accuracy: 46.32% Loss: 1.606\n",
      "Epoch: 28 Train accuracy: 49.09% Loss: 1.525\n",
      "Epoch: 29 Train accuracy: 53.26% Loss: 1.432\n",
      "Epoch: 30 Train accuracy: 54.66% Loss: 1.366\n",
      "Epoch: 31 Train accuracy: 58.51% Loss: 1.261\n",
      "Epoch: 32 Train accuracy: 60.68% Loss: 1.213\n",
      "Epoch: 33 Train accuracy: 61.99% Loss: 1.179\n",
      "Epoch: 34 Train accuracy: 63.64% Loss: 1.136\n",
      "Epoch: 35 Train accuracy: 63.42% Loss: 1.137\n",
      "Epoch: 36 Train accuracy: 64.49% Loss: 1.091\n",
      "Epoch: 37 Train accuracy: 65.13% Loss: 1.071\n",
      "Epoch: 38 Train accuracy: 65.71% Loss: 1.052\n",
      "Epoch: 39 Train accuracy: 66.19% Loss: 1.036\n",
      "Epoch: 40 Train accuracy: 66.52% Loss: 1.027\n",
      "Epoch: 41 Train accuracy: 66.83% Loss: 1.021\n",
      "Epoch: 42 Train accuracy: 65.99% Loss: 1.057\n",
      "Epoch: 43 Train accuracy: 64.56% Loss: 1.096\n",
      "Epoch: 44 Train accuracy: 65.15% Loss: 1.078\n",
      "Epoch: 45 Train accuracy: 65.43% Loss: 1.070\n",
      "Epoch: 46 Train accuracy: 66.16% Loss: 1.045\n",
      "Epoch: 47 Train accuracy: 67.31% Loss: 1.016\n",
      "Epoch: 48 Train accuracy: 67.47% Loss: 1.003\n",
      "Epoch: 49 Train accuracy: 67.64% Loss: 0.996\n",
      "Epoch: 50 Train accuracy: 60.13% Loss: 1.287\n",
      "Epoch: 51 Train accuracy: 61.21% Loss: 1.207\n",
      "Epoch: 52 Train accuracy: 62.96% Loss: 1.143\n",
      "Epoch: 53 Train accuracy: 65.39% Loss: 1.067\n",
      "Epoch: 54 Train accuracy: 65.78% Loss: 1.049\n",
      "Epoch: 55 Train accuracy: 65.86% Loss: 1.052\n",
      "Epoch: 56 Train accuracy: 66.75% Loss: 1.021\n",
      "Epoch: 57 Train accuracy: 66.69% Loss: 1.023\n",
      "Epoch: 58 Train accuracy: 66.96% Loss: 1.019\n",
      "Epoch: 59 Train accuracy: 68.35% Loss: 0.964\n",
      "Epoch: 60 Train accuracy: 68.82% Loss: 0.947\n",
      "Epoch: 61 Train accuracy: 68.35% Loss: 0.964\n",
      "Epoch: 62 Train accuracy: 67.46% Loss: 0.991\n",
      "Epoch: 63 Train accuracy: 66.69% Loss: 1.014\n",
      "Epoch: 64 Train accuracy: 68.17% Loss: 0.976\n",
      "Epoch: 65 Train accuracy: 68.33% Loss: 0.969\n",
      "Epoch: 66 Train accuracy: 68.12% Loss: 0.979\n",
      "Epoch: 67 Train accuracy: 68.69% Loss: 0.954\n",
      "Epoch: 68 Train accuracy: 69.45% Loss: 0.935\n",
      "Epoch: 69 Train accuracy: 69.50% Loss: 0.931\n",
      "Epoch: 70 Train accuracy: 69.10% Loss: 0.950\n",
      "Epoch: 71 Train accuracy: 68.20% Loss: 0.984\n",
      "Epoch: 72 Train accuracy: 68.67% Loss: 0.948\n",
      "Epoch: 73 Train accuracy: 68.85% Loss: 0.959\n",
      "Epoch: 74 Train accuracy: 68.12% Loss: 0.979\n",
      "Epoch: 75 Train accuracy: 62.20% Loss: 1.242\n",
      "Epoch: 76 Train accuracy: 67.17% Loss: 1.055\n",
      "Epoch: 77 Train accuracy: 67.67% Loss: 0.988\n",
      "Epoch: 78 Train accuracy: 68.86% Loss: 0.953\n",
      "Epoch: 79 Train accuracy: 69.37% Loss: 0.940\n",
      "Epoch: 80 Train accuracy: 69.07% Loss: 0.953\n",
      "Epoch: 81 Train accuracy: 68.69% Loss: 0.960\n",
      "Epoch: 82 Train accuracy: 68.80% Loss: 0.953\n",
      "Epoch: 83 Train accuracy: 68.96% Loss: 0.951\n",
      "Epoch: 84 Train accuracy: 68.54% Loss: 0.966\n",
      "Epoch: 85 Train accuracy: 67.87% Loss: 0.999\n",
      "Epoch: 86 Train accuracy: 68.06% Loss: 0.990\n",
      "Epoch: 87 Train accuracy: 67.90% Loss: 1.004\n",
      "Epoch: 88 Train accuracy: 68.83% Loss: 0.967\n",
      "Epoch: 89 Train accuracy: 69.46% Loss: 0.956\n",
      "Epoch: 90 Train accuracy: 69.92% Loss: 0.924\n",
      "Epoch: 91 Train accuracy: 70.14% Loss: 0.925\n",
      "Epoch: 92 Train accuracy: 69.81% Loss: 0.931\n",
      "Epoch: 93 Train accuracy: 70.00% Loss: 0.921\n",
      "Epoch: 94 Train accuracy: 70.31% Loss: 0.913\n",
      "Epoch: 95 Train accuracy: 70.75% Loss: 0.895\n",
      "Epoch: 96 Train accuracy: 70.36% Loss: 0.905\n",
      "Epoch: 97 Train accuracy: 70.22% Loss: 0.915\n",
      "Epoch: 98 Train accuracy: 70.07% Loss: 0.920\n",
      "Epoch: 99 Train accuracy: 69.94% Loss: 0.928\n",
      "Epoch: 100 Train accuracy: 64.67% Loss: 1.170\n",
      "Epoch: 101 Train accuracy: 68.71% Loss: 1.011\n",
      "Epoch: 102 Train accuracy: 68.53% Loss: 0.971\n",
      "Epoch: 103 Train accuracy: 69.16% Loss: 0.945\n",
      "Epoch: 104 Train accuracy: 70.29% Loss: 0.915\n",
      "Epoch: 105 Train accuracy: 70.22% Loss: 0.917\n",
      "Epoch: 106 Train accuracy: 70.24% Loss: 0.918\n",
      "Epoch: 107 Train accuracy: 70.43% Loss: 0.907\n",
      "Epoch: 108 Train accuracy: 70.04% Loss: 0.924\n",
      "Epoch: 109 Train accuracy: 68.97% Loss: 0.957\n",
      "Epoch: 110 Train accuracy: 69.18% Loss: 0.955\n",
      "Epoch: 111 Train accuracy: 70.28% Loss: 0.915\n",
      "Epoch: 112 Train accuracy: 70.49% Loss: 0.904\n",
      "Epoch: 113 Train accuracy: 70.55% Loss: 0.908\n",
      "Epoch: 114 Train accuracy: 70.00% Loss: 0.926\n",
      "Epoch: 115 Train accuracy: 69.38% Loss: 0.943\n",
      "Epoch: 116 Train accuracy: 70.03% Loss: 0.918\n",
      "Epoch: 117 Train accuracy: 70.48% Loss: 0.906\n",
      "Epoch: 118 Train accuracy: 70.41% Loss: 0.911\n",
      "Epoch: 119 Train accuracy: 70.31% Loss: 0.917\n",
      "Epoch: 120 Train accuracy: 70.10% Loss: 0.931\n",
      "Epoch: 121 Train accuracy: 70.20% Loss: 0.919\n",
      "Epoch: 122 Train accuracy: 70.80% Loss: 0.906\n",
      "Epoch: 123 Train accuracy: 70.75% Loss: 0.909\n",
      "Epoch: 124 Train accuracy: 70.71% Loss: 0.910\n",
      "Epoch: 125 Train accuracy: 66.57% Loss: 1.098\n",
      "Epoch: 126 Train accuracy: 70.68% Loss: 0.942\n",
      "Epoch: 127 Train accuracy: 70.62% Loss: 0.911\n",
      "Epoch: 128 Train accuracy: 71.67% Loss: 0.884\n",
      "Epoch: 129 Train accuracy: 71.28% Loss: 0.885\n",
      "Epoch: 130 Train accuracy: 71.58% Loss: 0.876\n",
      "Epoch: 131 Train accuracy: 71.05% Loss: 0.902\n",
      "Epoch: 132 Train accuracy: 70.88% Loss: 0.906\n",
      "Epoch: 133 Train accuracy: 70.85% Loss: 0.908\n",
      "Epoch: 134 Train accuracy: 70.53% Loss: 0.916\n",
      "Epoch: 135 Train accuracy: 70.21% Loss: 0.919\n",
      "Epoch: 136 Train accuracy: 69.86% Loss: 0.936\n",
      "Epoch: 137 Train accuracy: 70.55% Loss: 0.915\n",
      "Epoch: 138 Train accuracy: 70.59% Loss: 0.921\n",
      "Epoch: 139 Train accuracy: 70.21% Loss: 0.919\n",
      "Epoch: 140 Train accuracy: 70.84% Loss: 0.902\n",
      "Epoch: 141 Train accuracy: 71.39% Loss: 0.890\n",
      "Epoch: 142 Train accuracy: 71.23% Loss: 0.891\n",
      "Epoch: 143 Train accuracy: 71.06% Loss: 0.892\n",
      "Epoch: 144 Train accuracy: 71.19% Loss: 0.892\n",
      "Epoch: 145 Train accuracy: 70.59% Loss: 0.923\n",
      "Epoch: 146 Train accuracy: 71.02% Loss: 0.899\n",
      "Epoch: 147 Train accuracy: 70.85% Loss: 0.905\n",
      "Epoch: 148 Train accuracy: 70.49% Loss: 0.921\n",
      "Epoch: 149 Train accuracy: 70.72% Loss: 0.915\n",
      "Epoch: 150 Train accuracy: 66.27% Loss: 1.082\n",
      "Epoch: 151 Train accuracy: 71.19% Loss: 0.910\n",
      "Epoch: 152 Train accuracy: 71.72% Loss: 0.880\n",
      "Epoch: 153 Train accuracy: 71.89% Loss: 0.872\n",
      "Epoch: 154 Train accuracy: 71.42% Loss: 0.878\n",
      "Epoch: 155 Train accuracy: 71.02% Loss: 0.893\n",
      "Epoch: 156 Train accuracy: 70.23% Loss: 0.923\n",
      "Epoch: 157 Train accuracy: 70.92% Loss: 0.897\n",
      "Epoch: 158 Train accuracy: 71.32% Loss: 0.886\n",
      "Epoch: 159 Train accuracy: 71.33% Loss: 0.885\n",
      "Epoch: 160 Train accuracy: 71.03% Loss: 0.896\n",
      "Epoch: 161 Train accuracy: 70.80% Loss: 0.908\n",
      "Epoch: 162 Train accuracy: 70.85% Loss: 0.909\n",
      "Epoch: 163 Train accuracy: 70.95% Loss: 0.902\n",
      "Epoch: 164 Train accuracy: 71.21% Loss: 0.896\n",
      "Epoch: 165 Train accuracy: 70.50% Loss: 0.920\n",
      "Epoch: 166 Train accuracy: 70.25% Loss: 0.936\n",
      "Epoch: 167 Train accuracy: 70.61% Loss: 0.926\n",
      "Epoch: 168 Train accuracy: 70.62% Loss: 0.920\n",
      "Epoch: 169 Train accuracy: 70.99% Loss: 0.903\n",
      "Epoch: 170 Train accuracy: 71.01% Loss: 0.904\n",
      "Epoch: 171 Train accuracy: 71.47% Loss: 0.893\n",
      "Epoch: 172 Train accuracy: 72.25% Loss: 0.868\n",
      "Epoch: 173 Train accuracy: 72.03% Loss: 0.866\n",
      "Epoch: 174 Train accuracy: 72.42% Loss: 0.855\n",
      "Epoch: 175 Train accuracy: 67.59% Loss: 1.078\n",
      "Epoch: 176 Train accuracy: 71.67% Loss: 0.909\n",
      "Epoch: 177 Train accuracy: 71.85% Loss: 0.889\n",
      "Epoch: 178 Train accuracy: 72.37% Loss: 0.861\n",
      "Epoch: 179 Train accuracy: 72.23% Loss: 0.856\n",
      "Epoch: 180 Train accuracy: 72.08% Loss: 0.859\n",
      "Epoch: 181 Train accuracy: 71.77% Loss: 0.869\n",
      "Epoch: 182 Train accuracy: 71.15% Loss: 0.891\n",
      "Epoch: 183 Train accuracy: 70.61% Loss: 0.904\n",
      "Epoch: 184 Train accuracy: 70.89% Loss: 0.901\n",
      "Epoch: 185 Train accuracy: 70.41% Loss: 0.921\n",
      "Epoch: 186 Train accuracy: 70.41% Loss: 0.924\n",
      "Epoch: 187 Train accuracy: 70.83% Loss: 0.908\n",
      "Epoch: 188 Train accuracy: 70.29% Loss: 0.926\n",
      "Epoch: 189 Train accuracy: 70.21% Loss: 0.930\n",
      "Epoch: 190 Train accuracy: 71.51% Loss: 0.894\n",
      "Epoch: 191 Train accuracy: 71.89% Loss: 0.873\n",
      "Epoch: 192 Train accuracy: 71.43% Loss: 0.890\n",
      "Epoch: 193 Train accuracy: 71.50% Loss: 0.890\n",
      "Epoch: 194 Train accuracy: 71.64% Loss: 0.881\n",
      "Epoch: 195 Train accuracy: 71.82% Loss: 0.879\n",
      "Epoch: 196 Train accuracy: 72.08% Loss: 0.866\n",
      "Epoch: 197 Train accuracy: 71.86% Loss: 0.870\n",
      "Epoch: 198 Train accuracy: 72.01% Loss: 0.870\n",
      "Epoch: 199 Train accuracy: 71.76% Loss: 0.875\n",
      "Epoch: 200 Train accuracy: 68.84% Loss: 1.050\n",
      "Epoch: 201 Train accuracy: 72.19% Loss: 0.891\n",
      "Epoch: 202 Train accuracy: 72.32% Loss: 0.869\n",
      "Epoch: 203 Train accuracy: 72.27% Loss: 0.870\n",
      "Epoch: 204 Train accuracy: 72.36% Loss: 0.861\n",
      "Epoch: 205 Train accuracy: 72.08% Loss: 0.862\n",
      "Epoch: 206 Train accuracy: 71.39% Loss: 0.891\n",
      "Epoch: 207 Train accuracy: 71.32% Loss: 0.896\n",
      "Epoch: 208 Train accuracy: 71.50% Loss: 0.891\n",
      "Epoch: 209 Train accuracy: 71.89% Loss: 0.884\n",
      "Epoch: 210 Train accuracy: 71.32% Loss: 0.894\n",
      "Epoch: 211 Train accuracy: 70.73% Loss: 0.908\n",
      "Epoch: 212 Train accuracy: 70.93% Loss: 0.899\n",
      "Epoch: 213 Train accuracy: 71.29% Loss: 0.891\n",
      "Epoch: 214 Train accuracy: 71.44% Loss: 0.893\n",
      "Epoch: 215 Train accuracy: 71.71% Loss: 0.876\n",
      "Epoch: 216 Train accuracy: 71.85% Loss: 0.873\n",
      "Epoch: 217 Train accuracy: 72.17% Loss: 0.864\n",
      "Epoch: 218 Train accuracy: 72.21% Loss: 0.867\n",
      "Epoch: 219 Train accuracy: 72.33% Loss: 0.865\n",
      "Epoch: 220 Train accuracy: 72.00% Loss: 0.868\n",
      "Epoch: 221 Train accuracy: 72.08% Loss: 0.859\n",
      "Epoch: 222 Train accuracy: 71.85% Loss: 0.877\n",
      "Epoch: 223 Train accuracy: 71.65% Loss: 0.880\n",
      "Epoch: 224 Train accuracy: 71.81% Loss: 0.873\n",
      "Epoch: 225 Train accuracy: 67.40% Loss: 1.104\n",
      "Epoch: 226 Train accuracy: 71.34% Loss: 0.917\n",
      "Epoch: 227 Train accuracy: 72.22% Loss: 0.870\n",
      "Epoch: 228 Train accuracy: 72.49% Loss: 0.857\n",
      "Epoch: 229 Train accuracy: 72.32% Loss: 0.864\n",
      "Epoch: 230 Train accuracy: 72.22% Loss: 0.858\n",
      "Epoch: 231 Train accuracy: 71.76% Loss: 0.872\n",
      "Epoch: 232 Train accuracy: 71.03% Loss: 0.903\n",
      "Epoch: 233 Train accuracy: 70.15% Loss: 0.932\n",
      "Epoch: 234 Train accuracy: 70.89% Loss: 0.910\n",
      "Epoch: 235 Train accuracy: 71.09% Loss: 0.902\n",
      "Epoch: 236 Train accuracy: 71.39% Loss: 0.893\n",
      "Epoch: 237 Train accuracy: 71.57% Loss: 0.879\n",
      "Epoch: 238 Train accuracy: 71.97% Loss: 0.864\n",
      "Epoch: 239 Train accuracy: 71.86% Loss: 0.871\n",
      "Epoch: 240 Train accuracy: 71.52% Loss: 0.885\n",
      "Epoch: 241 Train accuracy: 71.76% Loss: 0.873\n",
      "Epoch: 242 Train accuracy: 71.68% Loss: 0.882\n",
      "Epoch: 243 Train accuracy: 71.98% Loss: 0.874\n",
      "Epoch: 244 Train accuracy: 72.42% Loss: 0.851\n",
      "Epoch: 245 Train accuracy: 72.06% Loss: 0.866\n",
      "Epoch: 246 Train accuracy: 71.85% Loss: 0.868\n",
      "Epoch: 247 Train accuracy: 71.92% Loss: 0.873\n",
      "Epoch: 248 Train accuracy: 71.97% Loss: 0.868\n",
      "Epoch: 249 Train accuracy: 72.24% Loss: 0.858\n",
      "Epoch: 250 Train accuracy: 68.73% Loss: 1.053\n",
      "Epoch: 251 Train accuracy: 71.71% Loss: 0.900\n",
      "Epoch: 252 Train accuracy: 71.73% Loss: 0.889\n",
      "Epoch: 253 Train accuracy: 71.99% Loss: 0.870\n",
      "Epoch: 254 Train accuracy: 71.83% Loss: 0.877\n",
      "Epoch: 255 Train accuracy: 71.96% Loss: 0.875\n",
      "Epoch: 256 Train accuracy: 71.14% Loss: 0.894\n",
      "Epoch: 257 Train accuracy: 71.36% Loss: 0.889\n",
      "Epoch: 258 Train accuracy: 70.89% Loss: 0.904\n",
      "Epoch: 259 Train accuracy: 70.64% Loss: 0.919\n",
      "Epoch: 260 Train accuracy: 70.43% Loss: 0.924\n",
      "Epoch: 261 Train accuracy: 71.29% Loss: 0.904\n",
      "Epoch: 262 Train accuracy: 71.10% Loss: 0.897\n",
      "Epoch: 263 Train accuracy: 71.12% Loss: 0.903\n",
      "Epoch: 264 Train accuracy: 71.53% Loss: 0.887\n",
      "Epoch: 265 Train accuracy: 71.38% Loss: 0.899\n",
      "Epoch: 266 Train accuracy: 71.38% Loss: 0.902\n",
      "Epoch: 267 Train accuracy: 71.25% Loss: 0.902\n",
      "Epoch: 268 Train accuracy: 71.49% Loss: 0.888\n",
      "Epoch: 269 Train accuracy: 71.91% Loss: 0.869\n",
      "Epoch: 270 Train accuracy: 72.34% Loss: 0.858\n",
      "Epoch: 271 Train accuracy: 72.17% Loss: 0.861\n",
      "Epoch: 272 Train accuracy: 71.80% Loss: 0.876\n",
      "Epoch: 273 Train accuracy: 71.71% Loss: 0.881\n",
      "Epoch: 274 Train accuracy: 71.85% Loss: 0.877\n",
      "Epoch: 275 Train accuracy: 68.11% Loss: 1.083\n",
      "Epoch: 276 Train accuracy: 71.13% Loss: 0.931\n",
      "Epoch: 277 Train accuracy: 71.60% Loss: 0.909\n",
      "Epoch: 278 Train accuracy: 71.67% Loss: 0.902\n",
      "Epoch: 279 Train accuracy: 71.59% Loss: 0.881\n",
      "Epoch: 280 Train accuracy: 72.11% Loss: 0.871\n",
      "Epoch: 281 Train accuracy: 71.59% Loss: 0.884\n",
      "Epoch: 282 Train accuracy: 71.42% Loss: 0.888\n",
      "Epoch: 283 Train accuracy: 70.98% Loss: 0.907\n",
      "Epoch: 284 Train accuracy: 71.21% Loss: 0.900\n",
      "Epoch: 285 Train accuracy: 71.51% Loss: 0.894\n",
      "Epoch: 286 Train accuracy: 71.75% Loss: 0.880\n",
      "Epoch: 287 Train accuracy: 71.56% Loss: 0.888\n",
      "Epoch: 288 Train accuracy: 71.99% Loss: 0.869\n",
      "Epoch: 289 Train accuracy: 71.21% Loss: 0.890\n",
      "Epoch: 290 Train accuracy: 71.85% Loss: 0.872\n",
      "Epoch: 291 Train accuracy: 71.77% Loss: 0.879\n",
      "Epoch: 292 Train accuracy: 71.91% Loss: 0.873\n",
      "Epoch: 293 Train accuracy: 72.13% Loss: 0.868\n",
      "Epoch: 294 Train accuracy: 72.10% Loss: 0.866\n",
      "Epoch: 295 Train accuracy: 71.57% Loss: 0.890\n",
      "Epoch: 296 Train accuracy: 71.43% Loss: 0.898\n",
      "Epoch: 297 Train accuracy: 71.62% Loss: 0.886\n",
      "Epoch: 298 Train accuracy: 72.03% Loss: 0.870\n",
      "Epoch: 299 Train accuracy: 72.11% Loss: 0.866\n",
      "Epoch: 300 Train accuracy: 68.78% Loss: 1.063\n",
      "Epoch: 301 Train accuracy: 71.42% Loss: 0.916\n",
      "Epoch: 302 Train accuracy: 71.90% Loss: 0.883\n",
      "Epoch: 303 Train accuracy: 71.89% Loss: 0.875\n",
      "Epoch: 304 Train accuracy: 71.92% Loss: 0.884\n",
      "Epoch: 305 Train accuracy: 71.88% Loss: 0.880\n",
      "Epoch: 306 Train accuracy: 71.82% Loss: 0.885\n",
      "Epoch: 307 Train accuracy: 71.59% Loss: 0.887\n",
      "Epoch: 308 Train accuracy: 71.31% Loss: 0.894\n",
      "Epoch: 309 Train accuracy: 70.80% Loss: 0.917\n",
      "Epoch: 310 Train accuracy: 71.25% Loss: 0.903\n",
      "Epoch: 311 Train accuracy: 71.67% Loss: 0.893\n",
      "Epoch: 312 Train accuracy: 71.55% Loss: 0.891\n",
      "Epoch: 313 Train accuracy: 71.52% Loss: 0.889\n",
      "Epoch: 314 Train accuracy: 72.03% Loss: 0.869\n",
      "Epoch: 315 Train accuracy: 71.93% Loss: 0.873\n",
      "Epoch: 316 Train accuracy: 71.54% Loss: 0.888\n",
      "Epoch: 317 Train accuracy: 71.61% Loss: 0.885\n",
      "Epoch: 318 Train accuracy: 71.47% Loss: 0.893\n",
      "Epoch: 319 Train accuracy: 71.63% Loss: 0.880\n",
      "Epoch: 320 Train accuracy: 71.91% Loss: 0.879\n",
      "Epoch: 321 Train accuracy: 72.45% Loss: 0.855\n",
      "Epoch: 322 Train accuracy: 72.53% Loss: 0.858\n",
      "Epoch: 323 Train accuracy: 72.46% Loss: 0.856\n",
      "Epoch: 324 Train accuracy: 72.13% Loss: 0.866\n",
      "Epoch: 325 Train accuracy: 68.09% Loss: 1.092\n",
      "Epoch: 326 Train accuracy: 69.40% Loss: 0.994\n",
      "Epoch: 327 Train accuracy: 71.67% Loss: 0.914\n",
      "Epoch: 328 Train accuracy: 72.44% Loss: 0.866\n",
      "Epoch: 329 Train accuracy: 72.13% Loss: 0.870\n",
      "Epoch: 330 Train accuracy: 71.81% Loss: 0.877\n",
      "Epoch: 331 Train accuracy: 71.40% Loss: 0.889\n",
      "Epoch: 332 Train accuracy: 71.36% Loss: 0.896\n",
      "Epoch: 333 Train accuracy: 71.46% Loss: 0.892\n",
      "Epoch: 334 Train accuracy: 70.94% Loss: 0.902\n",
      "Epoch: 335 Train accuracy: 71.00% Loss: 0.894\n",
      "Epoch: 336 Train accuracy: 71.39% Loss: 0.888\n",
      "Epoch: 337 Train accuracy: 72.17% Loss: 0.860\n",
      "Epoch: 338 Train accuracy: 72.37% Loss: 0.854\n",
      "Epoch: 339 Train accuracy: 71.83% Loss: 0.868\n",
      "Epoch: 340 Train accuracy: 72.02% Loss: 0.866\n",
      "Epoch: 341 Train accuracy: 71.96% Loss: 0.863\n",
      "Epoch: 342 Train accuracy: 71.44% Loss: 0.881\n",
      "Epoch: 343 Train accuracy: 71.60% Loss: 0.878\n",
      "Epoch: 344 Train accuracy: 71.92% Loss: 0.877\n",
      "Epoch: 345 Train accuracy: 71.98% Loss: 0.872\n",
      "Epoch: 346 Train accuracy: 71.69% Loss: 0.876\n",
      "Epoch: 347 Train accuracy: 71.57% Loss: 0.880\n",
      "Epoch: 348 Train accuracy: 71.71% Loss: 0.884\n",
      "Epoch: 349 Train accuracy: 71.78% Loss: 0.878\n",
      "Epoch: 350 Train accuracy: 70.20% Loss: 1.044\n",
      "Epoch: 351 Train accuracy: 72.41% Loss: 0.888\n",
      "Epoch: 352 Train accuracy: 72.55% Loss: 0.873\n",
      "Epoch: 353 Train accuracy: 72.35% Loss: 0.865\n",
      "Epoch: 354 Train accuracy: 72.15% Loss: 0.875\n",
      "Epoch: 355 Train accuracy: 72.10% Loss: 0.878\n",
      "Epoch: 356 Train accuracy: 71.18% Loss: 0.904\n",
      "Epoch: 357 Train accuracy: 71.27% Loss: 0.900\n",
      "Epoch: 358 Train accuracy: 71.29% Loss: 0.899\n",
      "Epoch: 359 Train accuracy: 71.35% Loss: 0.893\n",
      "Epoch: 360 Train accuracy: 71.78% Loss: 0.877\n",
      "Epoch: 361 Train accuracy: 72.19% Loss: 0.863\n",
      "Epoch: 362 Train accuracy: 71.69% Loss: 0.874\n",
      "Epoch: 363 Train accuracy: 72.02% Loss: 0.867\n",
      "Epoch: 364 Train accuracy: 71.86% Loss: 0.875\n",
      "Epoch: 365 Train accuracy: 71.89% Loss: 0.873\n",
      "Epoch: 366 Train accuracy: 71.43% Loss: 0.889\n",
      "Epoch: 367 Train accuracy: 71.56% Loss: 0.885\n",
      "Epoch: 368 Train accuracy: 72.22% Loss: 0.861\n",
      "Epoch: 369 Train accuracy: 72.45% Loss: 0.854\n",
      "Epoch: 370 Train accuracy: 72.40% Loss: 0.856\n",
      "Epoch: 371 Train accuracy: 71.82% Loss: 0.874\n",
      "Epoch: 372 Train accuracy: 72.17% Loss: 0.861\n",
      "Epoch: 373 Train accuracy: 72.34% Loss: 0.859\n",
      "Epoch: 374 Train accuracy: 72.12% Loss: 0.864\n",
      "Epoch: 375 Train accuracy: 70.46% Loss: 1.025\n",
      "Epoch: 376 Train accuracy: 72.29% Loss: 0.886\n",
      "Epoch: 377 Train accuracy: 72.53% Loss: 0.862\n",
      "Epoch: 378 Train accuracy: 72.57% Loss: 0.857\n",
      "Epoch: 379 Train accuracy: 72.30% Loss: 0.859\n",
      "Epoch: 380 Train accuracy: 72.15% Loss: 0.863\n",
      "Epoch: 381 Train accuracy: 71.65% Loss: 0.874\n",
      "Epoch: 382 Train accuracy: 71.30% Loss: 0.890\n",
      "Epoch: 383 Train accuracy: 70.80% Loss: 0.914\n",
      "Epoch: 384 Train accuracy: 71.14% Loss: 0.900\n",
      "Epoch: 385 Train accuracy: 71.52% Loss: 0.884\n",
      "Epoch: 386 Train accuracy: 72.17% Loss: 0.858\n",
      "Epoch: 387 Train accuracy: 72.50% Loss: 0.855\n",
      "Epoch: 388 Train accuracy: 72.46% Loss: 0.847\n",
      "Epoch: 389 Train accuracy: 72.35% Loss: 0.857\n",
      "Epoch: 390 Train accuracy: 72.70% Loss: 0.843\n",
      "Epoch: 391 Train accuracy: 72.61% Loss: 0.847\n",
      "Epoch: 392 Train accuracy: 72.46% Loss: 0.848\n",
      "Epoch: 393 Train accuracy: 72.11% Loss: 0.856\n",
      "Epoch: 394 Train accuracy: 71.82% Loss: 0.863\n",
      "Epoch: 395 Train accuracy: 71.94% Loss: 0.870\n",
      "Epoch: 396 Train accuracy: 72.22% Loss: 0.856\n",
      "Epoch: 397 Train accuracy: 72.56% Loss: 0.848\n",
      "Epoch: 398 Train accuracy: 72.42% Loss: 0.845\n",
      "Epoch: 399 Train accuracy: 72.22% Loss: 0.849\n",
      "Epoch: 400 Train accuracy: 69.83% Loss: 1.025\n",
      "Epoch: 401 Train accuracy: 72.16% Loss: 0.892\n",
      "Epoch: 402 Train accuracy: 72.87% Loss: 0.861\n",
      "Epoch: 403 Train accuracy: 73.00% Loss: 0.845\n",
      "Epoch: 404 Train accuracy: 72.45% Loss: 0.861\n",
      "Epoch: 405 Train accuracy: 72.12% Loss: 0.876\n",
      "Epoch: 406 Train accuracy: 71.77% Loss: 0.887\n",
      "Epoch: 407 Train accuracy: 71.68% Loss: 0.886\n",
      "Epoch: 408 Train accuracy: 71.36% Loss: 0.893\n",
      "Epoch: 409 Train accuracy: 71.71% Loss: 0.885\n",
      "Epoch: 410 Train accuracy: 71.90% Loss: 0.869\n",
      "Epoch: 411 Train accuracy: 72.11% Loss: 0.858\n",
      "Epoch: 412 Train accuracy: 72.40% Loss: 0.856\n",
      "Epoch: 413 Train accuracy: 71.75% Loss: 0.865\n",
      "Epoch: 414 Train accuracy: 71.61% Loss: 0.876\n",
      "Epoch: 415 Train accuracy: 72.14% Loss: 0.858\n",
      "Epoch: 416 Train accuracy: 72.18% Loss: 0.859\n",
      "Epoch: 417 Train accuracy: 72.10% Loss: 0.859\n",
      "Epoch: 418 Train accuracy: 72.32% Loss: 0.856\n",
      "Epoch: 419 Train accuracy: 72.14% Loss: 0.860\n",
      "Epoch: 420 Train accuracy: 71.77% Loss: 0.882\n",
      "Epoch: 421 Train accuracy: 71.77% Loss: 0.886\n",
      "Epoch: 422 Train accuracy: 71.73% Loss: 0.875\n",
      "Epoch: 423 Train accuracy: 72.65% Loss: 0.848\n",
      "Epoch: 424 Train accuracy: 72.95% Loss: 0.836\n",
      "Epoch: 425 Train accuracy: 70.32% Loss: 1.028\n",
      "Epoch: 426 Train accuracy: 72.37% Loss: 0.885\n",
      "Epoch: 427 Train accuracy: 72.83% Loss: 0.852\n",
      "Epoch: 428 Train accuracy: 72.81% Loss: 0.849\n",
      "Epoch: 429 Train accuracy: 72.92% Loss: 0.839\n",
      "Epoch: 430 Train accuracy: 72.77% Loss: 0.840\n",
      "Epoch: 431 Train accuracy: 72.06% Loss: 0.864\n",
      "Epoch: 432 Train accuracy: 71.89% Loss: 0.871\n",
      "Epoch: 433 Train accuracy: 71.32% Loss: 0.890\n",
      "Epoch: 434 Train accuracy: 71.56% Loss: 0.886\n",
      "Epoch: 435 Train accuracy: 71.05% Loss: 0.894\n",
      "Epoch: 436 Train accuracy: 71.52% Loss: 0.887\n",
      "Epoch: 437 Train accuracy: 71.78% Loss: 0.874\n",
      "Epoch: 438 Train accuracy: 71.67% Loss: 0.876\n",
      "Epoch: 439 Train accuracy: 71.80% Loss: 0.877\n",
      "Epoch: 440 Train accuracy: 72.18% Loss: 0.855\n",
      "Epoch: 441 Train accuracy: 72.12% Loss: 0.859\n",
      "Epoch: 442 Train accuracy: 72.10% Loss: 0.859\n",
      "Epoch: 443 Train accuracy: 72.35% Loss: 0.851\n",
      "Epoch: 444 Train accuracy: 72.21% Loss: 0.852\n",
      "Epoch: 445 Train accuracy: 72.23% Loss: 0.858\n",
      "Epoch: 446 Train accuracy: 71.97% Loss: 0.863\n",
      "Epoch: 447 Train accuracy: 72.34% Loss: 0.850\n",
      "Epoch: 448 Train accuracy: 72.04% Loss: 0.865\n",
      "Epoch: 449 Train accuracy: 72.30% Loss: 0.854\n",
      "Epoch: 450 Train accuracy: 70.56% Loss: 1.017\n",
      "Epoch: 451 Train accuracy: 72.42% Loss: 0.881\n",
      "Epoch: 452 Train accuracy: 72.28% Loss: 0.870\n",
      "Epoch: 453 Train accuracy: 72.46% Loss: 0.851\n",
      "Epoch: 454 Train accuracy: 72.48% Loss: 0.846\n",
      "Epoch: 455 Train accuracy: 72.58% Loss: 0.845\n",
      "Epoch: 456 Train accuracy: 72.08% Loss: 0.859\n",
      "Epoch: 457 Train accuracy: 72.06% Loss: 0.856\n",
      "Epoch: 458 Train accuracy: 71.86% Loss: 0.862\n",
      "Epoch: 459 Train accuracy: 72.18% Loss: 0.860\n",
      "Epoch: 460 Train accuracy: 71.40% Loss: 0.884\n",
      "Epoch: 461 Train accuracy: 72.19% Loss: 0.862\n",
      "Epoch: 462 Train accuracy: 72.13% Loss: 0.860\n",
      "Epoch: 463 Train accuracy: 72.11% Loss: 0.859\n",
      "Epoch: 464 Train accuracy: 72.40% Loss: 0.849\n",
      "Epoch: 465 Train accuracy: 72.72% Loss: 0.840\n",
      "Epoch: 466 Train accuracy: 72.86% Loss: 0.835\n",
      "Epoch: 467 Train accuracy: 72.54% Loss: 0.846\n",
      "Epoch: 468 Train accuracy: 72.22% Loss: 0.855\n",
      "Epoch: 469 Train accuracy: 71.83% Loss: 0.861\n",
      "Epoch: 470 Train accuracy: 72.23% Loss: 0.852\n",
      "Epoch: 471 Train accuracy: 72.34% Loss: 0.849\n",
      "Epoch: 472 Train accuracy: 72.26% Loss: 0.853\n",
      "Epoch: 473 Train accuracy: 72.39% Loss: 0.845\n",
      "Epoch: 474 Train accuracy: 72.54% Loss: 0.845\n",
      "Epoch: 475 Train accuracy: 70.51% Loss: 1.009\n",
      "Epoch: 476 Train accuracy: 72.29% Loss: 0.881\n",
      "Epoch: 477 Train accuracy: 72.96% Loss: 0.847\n",
      "Epoch: 478 Train accuracy: 73.01% Loss: 0.837\n",
      "Epoch: 479 Train accuracy: 72.39% Loss: 0.853\n",
      "Epoch: 480 Train accuracy: 72.19% Loss: 0.854\n",
      "Epoch: 481 Train accuracy: 71.75% Loss: 0.869\n",
      "Epoch: 482 Train accuracy: 71.81% Loss: 0.874\n",
      "Epoch: 483 Train accuracy: 71.39% Loss: 0.886\n",
      "Epoch: 484 Train accuracy: 71.49% Loss: 0.877\n",
      "Epoch: 485 Train accuracy: 71.73% Loss: 0.877\n",
      "Epoch: 486 Train accuracy: 71.09% Loss: 0.889\n",
      "Epoch: 487 Train accuracy: 71.70% Loss: 0.875\n",
      "Epoch: 488 Train accuracy: 72.12% Loss: 0.857\n",
      "Epoch: 489 Train accuracy: 72.21% Loss: 0.858\n",
      "Epoch: 490 Train accuracy: 72.06% Loss: 0.855\n",
      "Epoch: 491 Train accuracy: 72.24% Loss: 0.853\n",
      "Epoch: 492 Train accuracy: 72.23% Loss: 0.849\n",
      "Epoch: 493 Train accuracy: 72.40% Loss: 0.843\n",
      "Epoch: 494 Train accuracy: 72.01% Loss: 0.852\n",
      "Epoch: 495 Train accuracy: 72.25% Loss: 0.848\n",
      "Epoch: 496 Train accuracy: 72.15% Loss: 0.856\n",
      "Epoch: 497 Train accuracy: 72.21% Loss: 0.861\n",
      "Epoch: 498 Train accuracy: 72.09% Loss: 0.861\n",
      "Epoch: 499 Train accuracy: 72.25% Loss: 0.847\n"
     ]
    }
   ],
   "source": [
    "## VFL Implementation above\n",
    "\n",
    "## HFL part follows\n",
    "\n",
    "\n",
    "EPOCHS = 500\n",
    "BATCH_SIZE = 60\n",
    "bottom_models = [BottomModel(7, 32)]*4\n",
    "final_out_dims = 10\n",
    "\n",
    "Network= VFLNetwork(bottom_models, final_out_dims)\n",
    "trainsets_with_splits= [[dataA_label1, dataB_label1, dataC_label1, dataD_label1], [dataA_label2, dataB_label2, dataC_label2, dataD_label2], [dataA_label3, dataB_label3, dataC_label3, dataD_label3], [dataA_label4, dataB_label4, dataC_label4, dataD_label4], [dataA_label5, dataB_label5, dataC_label5, dataD_label5]]\n",
    "train_label_set_split= [labels1, labels2, labels3, labels4, labels5]\n",
    "Network.train_with_settings(EPOCHS, BATCH_SIZE, trainsets_with_splits, train_label_set_split)\n",
    "\n",
    "testset_with_splits= [[testA_1, testB_1, testC_1, testD_1], [testA_2, testB_2, testC_2, testD_2], [testA_3, testB_3, testC_3, testD_3], [testA_4, testB_4, testC_4, testD_4], [testA_5, testB_5, testC_5, testD_5]]\n",
    "test_label_set_split= [test_labels1, test_labels2, test_labels3, test_labels4, test_labels5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of 1st top model: 36.20\n",
      "Test accuracy of 2nd top model: 35.15\n",
      "Test accuracy of 3rd top model: 36.35\n",
      "Test accuracy of 4th top model: 34.75\n",
      "Test accuracy of 5th top model: 35.10\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test accuracy of 1st top model: {Network.test(testset_with_splits, test_label_set_split, 0)[0]*100:.2f}\")\n",
    "print(f\"Test accuracy of 2nd top model: {Network.test(testset_with_splits, test_label_set_split, 1)[0]*100:.2f}\")\n",
    "print(f\"Test accuracy of 3rd top model: {Network.test(testset_with_splits, test_label_set_split, 2)[0]*100:.2f}\")\n",
    "print(f\"Test accuracy of 4th top model: {Network.test(testset_with_splits, test_label_set_split, 3)[0]*100:.2f}\")\n",
    "print(f\"Test accuracy of 5th top model: {Network.test(testset_with_splits, test_label_set_split, 4)[0]*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     # model architecture hyperparameters\n",
    "#     outs_per_client = 10\n",
    "#     bottom_models = [BottomModel(7, 32)]*4\n",
    "#     final_out_dims = 10\n",
    "#     Network = VFLNetwork(bottom_models, final_out_dims)\n",
    "\n",
    "#     #Training configurations\n",
    "#     EPOCHS = 500\n",
    "#     BATCH_SIZE = 64\n",
    "#     Network.train_with_settings(EPOCHS, BATCH_SIZE, [dataA_label1, dataB_label1, dataC_label1, dataD_label1], labels1)\n",
    "#     accuracy, loss = Network.test([test1, test2, test3, test4], test_labels)\n",
    "#         # test_loss.append(loss)\n",
    "#         # epoch_nums.append((i+1)*20)\n",
    "\n",
    "    \n",
    "#     print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "#     accuracy_at_each_epoch= np.array(accuracy_at_each_epoch)\n",
    "#     plt.plot(accuracy_at_each_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function for running F epochs of local training\n",
    "\n",
    "# def train_F_epochs(F: int, Network1: VFLNetwork, Network2: VFLNetwork, Network3: VFLNetwork, Network4: VFLNetwork, Network5: VFLNetwork):\n",
    "#     Network1.train_with_settings(F, BATCH_SIZE, [dataA_label1, dataB_label1, dataC_label1, dataD_label1], labels1)\n",
    "#     Network2.train_with_settings(F, BATCH_SIZE, [dataA_label2, dataB_label2, dataC_label2, dataD_label2], labels2)\n",
    "#     Network3.train_with_settings(F, BATCH_SIZE, [dataA_label3, dataB_label3, dataC_label3, dataD_label3], labels3)\n",
    "#     Network4.train_with_settings(F, BATCH_SIZE, [dataA_label4, dataB_label4, dataC_label4, dataD_label4], labels4)\n",
    "#     Network5.train_with_settings(F, BATCH_SIZE, [dataA_label5, dataB_label5, dataC_label5, dataD_label5], labels5)\n",
    "    \n",
    "#     local_parameters = []\n",
    "#     for network in [Network1, Network2, Network3, Network4, Network5]:\n",
    "#         local_parameters.append(network.state_dict())\n",
    "    \n",
    "#     return local_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Server level model\n",
    "\n",
    "# server_bottom_models= [BottomModel(7, 32)]*4\n",
    "# server_top_model= TopModel(server_bottom_models, 10)\n",
    "\n",
    "# total_epochs= 10\n",
    "# local_epochs= 50\n",
    "\n",
    "# for i in range(total_epochs):\n",
    "#     parameters= train_F_epochs(local_epochs, Network1, Network2, Network3, Network4, Network5)\n",
    "#     avg_parameters= OrderedDict()\n",
    "#     for key in parameters[0]:\n",
    "#         avg_parameters[key]= (parameters[0][key]+parameters[1][key]+parameters[2][key]+parameters[3][key] + parameters[4])/5\n",
    "#     Network1.load_state_dict(avg_parameters)\n",
    "#     Network2.load_state_dict(avg_parameters)\n",
    "#     Network3.load_state_dict(avg_parameters)\n",
    "#     Network4.load_state_dict(avg_parameters)\n",
    "#     Network5.load_state_dict(avg_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
