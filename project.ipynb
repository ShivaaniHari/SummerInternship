{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the MNIST dataset to work on\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "data_path_str = \"./data\"\n",
    "ETA = \"\\N{GREEK SMALL LETTER ETA}\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.deterministic=True\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # normalize by training set mean and standard deviation\n",
    "    # resulting data has mean=0 and std=1\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = MNIST(data_path_str, train=True, download=True, transform=transform)\n",
    "test_loader = DataLoader(\n",
    "    MNIST(data_path_str, train=False, download=False, transform=transform),\n",
    "    # decrease batch size if running into memory issues when testing\n",
    "    # a bespoke generator is passed to avoid reproducibility issues\n",
    "    shuffle=False, drop_last=False, batch_size=10000, generator=torch.Generator())\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning data (each image into 4 parts)\n",
    "\n",
    "data1= torch.stack([a[0:7]/255 for a in train_dataset.data])\n",
    "data2= torch.stack([a[7:14]/255 for a in train_dataset.data])\n",
    "data3= torch.stack([a[14:21]/255 for a in train_dataset.data])\n",
    "data4= torch.stack([a[21:28]/255 for a in train_dataset.data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "\n",
    "test_dataset= [test_loader.dataset[i][0] for i in range(len(test_loader.dataset))]\n",
    "test_labels= [test_loader.dataset[i][1] for i in range(len(test_loader.dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating label owner split\n",
    "from typing import cast\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def split(nr_clients: int, seed: int) -> list[Subset]:\n",
    "    rng = npr.default_rng(seed)\n",
    "    indices= rng.permutation(len(train_dataset))\n",
    "    splits = np.array_split(indices, nr_clients)\n",
    "\n",
    "    return [Subset(train_dataset, split) for split in cast(list[list[int]], splits)], indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating label split\n",
    "sample_split, sample_ids= split(5, 42)\n",
    "label_owner1= sample_split[0]\n",
    "label_id1= sample_ids[0:12000]\n",
    "\n",
    "# Aligning the data across each of the owners and label owner 1\n",
    "# Retrieving data corresponding to which labels are with label owner 1\n",
    "\n",
    "labels1= [label_owner1[i][1] for i in range(len(label_owner1))]\n",
    "dataA_label1= torch.stack([data1[i] for i in label_id1])\n",
    "dataB_label1= torch.stack([data2[i] for i in label_id1])\n",
    "dataC_label1= torch.stack([data3[i] for i in label_id1])\n",
    "dataD_label1= torch.stack([data4[i] for i in label_id1])\n",
    "data_labels1= [dataA_label1, dataB_label1, dataC_label1, dataD_label1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Data owner neural network\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BottomModel(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat):\n",
    "        super(BottomModel, self).__init__()\n",
    "        self.local_out_dim = out_feat\n",
    "        self.conv= nn.Conv2d(in_feat, 32, 3, 1)\n",
    "    \n",
    "    def forward(self, x:torch.tensor):\n",
    "        x= x.transpose(0, 1)\n",
    "        x= self.conv(x)\n",
    "        x= x.transpose(0, 1)\n",
    "        x= F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label owner neural network\n",
    "\n",
    "class TopModel(nn.Module):\n",
    "    def __init__(self, local_models, n_outs):\n",
    "        super(TopModel, self).__init__()\n",
    "        self.in_size = sum([local_models[i].local_out_dim for i in range(len(local_models))])\n",
    "        self.conv = nn.Conv2d(32, 128, 3, 1)\n",
    "        self.lin1 = nn.Linear(128, 256)\n",
    "        self.lin2 = nn.Linear(256, 10) # Final output = number of possible classes (10 digit types)\n",
    "        self.act = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        concat_outs = torch.cat(x, dim=1)  # concatenate local model outputs before forward pass\n",
    "        x = self.act(self.conv(concat_outs))\n",
    "        x = self.act(self.lin1(x))\n",
    "        x = self.act(self.lin2(x))\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VFLNetwork(nn.Module):\n",
    "    def __init__(self, local_models, n_outs):\n",
    "        super(VFLNetwork, self).__init__()\n",
    "        self.bottom_models = local_models\n",
    "        self.top_model = TopModel(self.bottom_models, n_outs)\n",
    "        self.optimizer = optim.AdamW(self.parameters())\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def train_with_settings(self, epochs, batch_sz, x, y):\n",
    "        num_batches = len(x[0]) // batch_sz if len(x[0]) % batch_sz == 0 else len(x[0]) // batch_sz + 1\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss = 0.0\n",
    "            correct = 0.0\n",
    "            total = 0.0\n",
    "            for minibatch in range(num_batches):\n",
    "                if minibatch == num_batches - 1:\n",
    "                    x_minibatch = [p[int(minibatch * batch_sz):] for p in x]\n",
    "                    y_minibatch = y[int(minibatch * batch_sz):]\n",
    "                else:\n",
    "                    x_minibatch = [p[int(minibatch * batch_sz):int((minibatch + 1) * batch_sz)] for p in x]\n",
    "                    y_minibatch = y[int(minibatch * batch_sz):int((minibatch + 1) * batch_sz)]\n",
    "\n",
    "                \n",
    "                outs = self.forward(x_minibatch)\n",
    "                pred = torch.argmax(outs, dim=1)\n",
    "                actual = torch.argmax(y_minibatch, dim=1)\n",
    "                correct += torch.sum((pred == actual))\n",
    "                total += len(actual)\n",
    "                loss = self.criterion(outs, y_minibatch)\n",
    "                total_loss += loss\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            print(\n",
    "                f\"Epoch: {epoch} Train accuracy: {correct * 100 / total:.2f}% Loss: {total_loss.detach().numpy()/num_batches:.3f}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        local_outs = [self.bottom_models[i](x[i]) for i in range(len(self.bottom_models))]\n",
    "        return self.top_model(local_outs)\n",
    "\n",
    "    def test(self, x, y):\n",
    "        with torch.no_grad():\n",
    "            outs = self.forward(x)\n",
    "            preds = torch.argmax(outs, dim=1)\n",
    "            actual = torch.argmax(y, dim=1)\n",
    "            accuracy = torch.sum((preds == actual)) / len(actual)\n",
    "            loss = self.criterion(outs, y)\n",
    "            return accuracy, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 32, 3, 3], expected input[1, 62, 128, 26] to have 32 channels, but got 62 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m     11\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_with_settings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataA_label1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataB_label1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataC_label1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataD_label1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_labels1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m accuracy, loss \u001b[38;5;241m=\u001b[39m Network\u001b[38;5;241m.\u001b[39mtest(test_dataset, test_labels)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[73], line 25\u001b[0m, in \u001b[0;36mVFLNetwork.train_with_settings\u001b[1;34m(self, epochs, batch_sz, x, y)\u001b[0m\n\u001b[0;32m     21\u001b[0m     x_minibatch \u001b[38;5;241m=\u001b[39m [p[\u001b[38;5;28mint\u001b[39m(minibatch \u001b[38;5;241m*\u001b[39m batch_sz):\u001b[38;5;28mint\u001b[39m((minibatch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_sz)] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m x]\n\u001b[0;32m     22\u001b[0m     y_minibatch \u001b[38;5;241m=\u001b[39m y[\u001b[38;5;28mint\u001b[39m(minibatch \u001b[38;5;241m*\u001b[39m batch_sz):\u001b[38;5;28mint\u001b[39m((minibatch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_sz)]\n\u001b[1;32m---> 25\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_minibatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     27\u001b[0m actual \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(y_minibatch, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[73], line 40\u001b[0m, in \u001b[0;36mVFLNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     39\u001b[0m     local_outs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbottom_models[i](x[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbottom_models))]\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_outs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shiva\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shiva\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[72], line 15\u001b[0m, in \u001b[0;36mTopModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     14\u001b[0m     concat_outs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# concatenate local model outputs before forward pass\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcat_outs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin1(x))\n\u001b[0;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin2(x))\n",
      "File \u001b[1;32mc:\\Users\\shiva\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shiva\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shiva\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shiva\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 32, 3, 3], expected input[1, 62, 128, 26] to have 32 channels, but got 62 channels instead"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # model architecture hyperparameters\n",
    "    outs_per_client = 10\n",
    "    bottom_models = [BottomModel(7, 70)]*4\n",
    "    final_out_dims = 10\n",
    "    Network = VFLNetwork(bottom_models, final_out_dims)\n",
    "\n",
    "    #Training configurations\n",
    "    EPOCHS = 500\n",
    "    BATCH_SIZE = 64\n",
    "    Network.train_with_settings(EPOCHS, BATCH_SIZE, [dataA_label1, dataB_label1, dataC_label1, dataD_label1], data_labels1)\n",
    "\n",
    "    \n",
    "    accuracy, loss = Network.test(test_dataset, test_labels)\n",
    "    print(f\"Test accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    # model architecture hyperparameters\n",
    "    outs_per_client = 2\n",
    "    bottom_models = [BottomModel(len(in_feats), outs_per_client * len(in_feats)) for in_feats in client_feature_names]\n",
    "    final_out_dims = 2\n",
    "    Network = VFLNetwork(bottom_models, final_out_dims)\n",
    "\n",
    "    #Training configurations\n",
    "    EPOCHS = 500\n",
    "    BATCH_SIZE = 64\n",
    "    TRAIN_TEST_THRESH = 0.8\n",
    "    X_train, X_test = X.loc[:int(TRAIN_TEST_THRESH * len(X))], X.loc[int(TRAIN_TEST_THRESH * len(X)) + 1:]\n",
    "    Y_train, Y_test = Y.loc[:int(TRAIN_TEST_THRESH * len(Y))], Y.loc[int(TRAIN_TEST_THRESH * len(Y)) + 1:]\n",
    "    Network.train_with_settings(EPOCHS, BATCH_SIZE, num_clients,\n",
    "                                client_feature_names, X_train, Y_train)\n",
    "\n",
    "    \n",
    "    accuracy, loss = Network.test(X_test, Y_test)\n",
    "    print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # Creating label owner split\n",
    "# from typing import cast\n",
    "\n",
    "# import numpy as np\n",
    "# import numpy.random as npr\n",
    "# from torch.utils.data import Subset\n",
    "\n",
    "# # Enter number of label owners in which data is to be partitioned, and the number that receive iid data\n",
    "# def split(nr_clients: int, iid: int, seed: int) -> list[Subset]:\n",
    "#     rng = npr.default_rng(seed)\n",
    "\n",
    "#     if iid==nr_clients:\n",
    "#         splits = np.array_split(rng.permutation(len(train_dataset)), nr_clients)\n",
    "#         reduced_splits= [split[:5000] for split in splits]\n",
    "#     else:\n",
    "#         if iid==4:\n",
    "#             # Creating 1 label owner with only 0 and 1 data points, rest receive iid\n",
    "#             class1 = [0, 1]\n",
    "#             split1 = [i for i, (e, c) in enumerate(train_dataset) if c in class1]\n",
    "#             split1= split1[:5000]\n",
    "#             splits= np.array_split(rng.permutation(len(train_dataset)), iid)\n",
    "#             reduced_splits= [split[:5000] for split in splits]\n",
    "#             reduced_splits.append(split1)\n",
    "\n",
    "#         elif iid==3:\n",
    "#             class1= [0, 1]\n",
    "#             class2= [2, 3]\n",
    "#             split1= [i for i, (e, c) in enumerate(train_dataset) if c in class1]\n",
    "#             split2= [i for i, (e, c) in enumerate(train_dataset) if c in class2]\n",
    "#             split1= split1[:5000]\n",
    "#             split2= split2[:5000]\n",
    "#             splits= np.array_split(rng.permutation(len(train_dataset)), iid)\n",
    "#             reduced_splits= [split[:5000] for split in splits]\n",
    "#             reduced_splits.append(split1)\n",
    "#             reduced_splits.append(split2)\n",
    "        \n",
    "#         elif iid==2:\n",
    "#             class1= [0, 1]\n",
    "#             class2= [2, 3]\n",
    "#             class3= [4, 5]\n",
    "#             split1= [i for i, (e, c) in enumerate(train_dataset) if c in class1]\n",
    "#             split2= [i for i, (e, c) in enumerate(train_dataset) if c in class2]\n",
    "#             split3= [i for i, (e, c) in enumerate(train_dataset) if c in class3]\n",
    "#             split1= split1[:5000]\n",
    "#             split2= split2[:5000]\n",
    "#             split3= split3[:5000]\n",
    "#             splits= np.array_split(rng.permutation(len(train_dataset)), iid)\n",
    "#             reduced_splits= [split[:5000] for split in splits]\n",
    "#             reduced_splits.append(split1)\n",
    "#             reduced_splits.append(split2)\n",
    "#             reduced_splits.append(split3)\n",
    "        \n",
    "#         elif iid==1:\n",
    "#             class1= [0, 1]\n",
    "#             class2= [2, 3]\n",
    "#             class3= [4, 5]\n",
    "#             class4= [6, 7]\n",
    "#             split1= [i for i, (e, c) in enumerate(train_dataset) if c in class1]\n",
    "#             split2= [i for i, (e, c) in enumerate(train_dataset) if c in class2]\n",
    "#             split3= [i for i, (e, c) in enumerate(train_dataset) if c in class3]\n",
    "#             split4= [i for i, (e, c) in enumerate(train_dataset) if c in class4]\n",
    "#             split1= split1[:5000]\n",
    "#             split2= split2[:5000]\n",
    "#             split3= split3[:5000]\n",
    "#             split4= split4[:5000]\n",
    "#             splits= np.array_split(rng.permutation(len(train_dataset)), iid)\n",
    "#             reduced_splits= [split[:5000] for split in splits]\n",
    "#             reduced_splits.append(split1)\n",
    "#             reduced_splits.append(split2)\n",
    "#             reduced_splits.append(split3)\n",
    "#             reduced_splits.append(split4)\n",
    "\n",
    "#     return [Subset(train_dataset, split) for split in cast(list[list[int]], reduced_splits)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # First type of split to train model on, iid distribution into 5 parts, for 5 label owners\n",
    "# sample_split1= split(5, 5, 42)\n",
    "\n",
    "# # Second type of split, 4 iid, 1 not\n",
    "# sample_split2= split(5, 4, 42)\n",
    "\n",
    "# # Third type of split, 3 iid\n",
    "# sample_split3= split(5, 3, 42)\n",
    "\n",
    "# # Fourth type of split, 2 iid\n",
    "# sample_split4= split(5, 2, 42)\n",
    "\n",
    "# # Fifth type of split, 1 iid\n",
    "# sample_split5= split(5, 1, 42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
